
\subsection{Factorise kernel codes with templates}

During the refactoring of the library code, we have to factorise the code from the kernels which have very similar implementation. Many similar kernels are not easy to maintain, as a single change have to be repeated many times in different kernels. However, it is not viable to directly make a generic kernel with many \verb|if - then - else| conditions, since it would slow down the execution. 
% This is because GPU execution runs the same instructions for all threads in an ensemble of threads called warp. For most NVIDIA GPUs, warps are made of 32 threads. Introducing conditions would make a divergence between the threads in the warp, creating different contexts and not running threads concurrently, making the whole program much slower.

The solution we adopted consists in using C++ templates with light use of meta-programming~\cite{wikibook:tmp} to generate multiple specialised version of a kernel at compilation time. We can then write branches in a clean fashion that would be resolved by the compiler. However, we cannot make any conditions on integer values from the get-go: since templates are resolved at compilation, the compiler can only resolve symbols on classes and types, and not values. 

We then use a small template as shown on Listing~\ref{lst:tmp} to derive our own types from integer values. We first define a template structure \verb|Int2Type| with inside an enumeration with a single value, equal to the one given for its template instantiation. The goal is to instantiate empty structures which are seen distinctly by the compiler. Hence, at compilation time, \verb|Int2Type<0>| is a structure name, \verb|Int2Type<1>| is another structure, and so on.

We then create a template structure \verb|SameType| that can be instantiated in two ways: if two objects of different types are passed (line 5), it bears an enumeration with value 0, and if types are the same (line 9), it bears value 1. Writing \verb|if| blocks using the \verb|SAMETYPE(a, b)| macro will expand it into 0 or 1, allowing the compiler to prune the unused branch automatically. Note that a template \verb|std::is_same<X, Y>| is already available to achieve the same goal in the C++ standard library; but we could not use it in CUDA kernels since this library is not implemented there.

\begin{listing}[h!]
	\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	fontsize=\footnotesize,
	linenos,
	breaklines,
	frame=single]{C++}
template <int Val>
struct Int2Type{
	enum {val_ = Val} dummy;
};
template<typename X, typename Y>
struct SameType {
	enum { result = 0 };
};
template<typename T>
struct SameType<T, T> {
	enum { result = 1 };
};
#define SAMETYPE(a, b) (SameType<a,b>::result)
	\end{minted}

	
	\caption{Meta-programming template to derive types from integer values}
	\label{lst:tmp}
\end{listing}
	
Finally, for all the kernels to be generated, explicit calls must be written in the source code. We decided to group all the kernels in three major kernel templates. Each of them can specialise to various extent:

\begin{itemize}
	\item global, that does not have any specialisation,
	\item local, on which we can select among 4 combinations:
	\begin{itemize}
		\item start position calculation (on / off),
		\item second-best score calculation (on / off);
	\end{itemize}
	\item semi-global, on which we can select among 64 combinations:
	\begin{itemize}
		\item if we allow to skip the beginning, the end, both ends or none of the ends of either query or target (making it 16 different cases),
		\item start position calculation (on / off),
		\item second-best score calculation (on / off).
	\end{itemize}
\end{itemize}

It would be highly unpractical to list all these variations, but we have written them before the compilation phase, where the templates are resolved. Subsequently, we rely on the C++ preprocessor and used a series of macro expansion to develop a \verb|switch - case| including all variations. This allows to write each kernel call exactly once, and condense all calls in a clean way.

\subsection{Porting newer version of GASAL2 in GASE}

Several modifications to BWA-MEM are done to integrate GASAL2 for its extension step. While some of them are already done when starting this thesis, substantial changes are still required to successfully integrate GASAL2 in BWA-MEM. 
GASAL2 aligns batches of sequences, and with a given batch of $k$ alignments, it instantiates $k$ threads. Each thread runs the alignment kernel on for its own pair of query and target on the GPU. Before launching a kernel, we need to provide to the GPU a batch of data to process in parallel, so we have to spread all the alignments in batches. This batching system was already implemented in a custom version of BWA for measurements called GASE-GASAL2~\cite{Ahmed:gase-gasal2}~\cite{Ahmed:GASE}. 

Another important change in GASE-GASAL2 is about seed chaining. We reviewed earlier that seeds are processed one after the other, so that if a seed is found in an alignment, it is skipped and not extended. Since GASE-GASAL2 processes the alignments in batches, it cannot later filter seeds based on the actual result of their alignments. The solution implemented to circumvent this issue is to estimate that all alignments for all seeds result in a 85\% coverage of the read. With this estimation, it filters seeds that are present in a range around the seed equal to 85\% of the length of the read. This feature is necessary for a parallell aligner to be integrated in BWA-MEM: we cannot provide seed filtering based on the previous calculations, so we use an estimation that covers most cases. We keep this feature unchanged for our own implementation.

GASE-GASAL2 only uses standard local alignment kernel of GASAL2 and does not provide same results as in original BWA-MEM. It has only been developed to demonstrate how seed-extension could be accelerated. It used an old version of GASAL2 which became incompatible with the changes we made previously.

The first task has been to update GASE-GASAL2 to compile it with the newer version of GASAL2. In particular, the source code had to be ported to C++. Several wrong writing practices had to be fixed, including naming variables ``or" (which is a keyword in C++), and defining clear context for function prototypes in the header files. In fact, C++ features name mangling for functions, which adds context information at compilation to establish contexts where function names are declared valid. This feature is implemented among others for function overloading. Unless plain C functions are placed in \verb|extern 'C' { ... }| blocks, they cannot be resolved at compilation, inducing a linking error, so some code clean-up had to be done on this side.